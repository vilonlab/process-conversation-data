{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e234d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Lowercase and remove punctuation.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{re.escape(punctuation)}]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def process_conversation(conv_text: str):\n",
    "    \"\"\"\n",
    "    Process conversation text into two numeric speaker streams for CRQA.\n",
    "    Dynamically detects the first two speakers and assigns word IDs.\n",
    "    \"\"\"\n",
    "    lines = [l.strip() for l in conv_text.split(\"\\n\") if l.strip()]\n",
    "    if not lines:\n",
    "        return {}\n",
    "\n",
    "    # Identify speakers dynamically\n",
    "    speakers = []\n",
    "    word_map = {}\n",
    "    next_id = 1\n",
    "    streams = {}\n",
    "\n",
    "    # These will be populated later once we know speaker order\n",
    "    speaker1, speaker2 = None, None\n",
    "    s1_words, s2_words = [], []\n",
    "\n",
    "    for line in lines:\n",
    "        if \":\" not in line:\n",
    "            continue\n",
    "\n",
    "        speaker, text = line.split(\":\", 1)\n",
    "        speaker = speaker.strip()\n",
    "        text = clean_text(text)\n",
    "        words = text.split()\n",
    "\n",
    "        # Register speakers in order of first appearance\n",
    "        if speaker not in speakers:\n",
    "            speakers.append(speaker)\n",
    "        if len(speakers) > 2:\n",
    "            raise ValueError(f\"More than two speakers found: {speakers}\")\n",
    "\n",
    "        if len(speakers) == 2:\n",
    "            speaker1, speaker2 = speakers\n",
    "\n",
    "        # Assign numeric IDs to new words\n",
    "        encoded = []\n",
    "        for w in words:\n",
    "            if w not in word_map:\n",
    "                word_map[w] = next_id\n",
    "                next_id += 1\n",
    "            encoded.append(word_map[w])\n",
    "\n",
    "        # Build parallel numeric sequences\n",
    "        if speaker == speakers[0]:\n",
    "            s1_words.extend(encoded)\n",
    "            s2_words.extend([None] * len(encoded))  # placeholder for listener\n",
    "        elif len(speakers) > 1 and speaker == speakers[1]:\n",
    "            s1_words.extend([None] * len(encoded))\n",
    "            s2_words.extend(encoded)\n",
    "\n",
    "    # Replace None with sentinel values for CRQA (-1/-2)\n",
    "    s1_filled = [x if x is not None else -1 for x in s1_words]\n",
    "    s2_filled = [x if x is not None else -2 for x in s2_words]\n",
    "\n",
    "    # Maintain original speaker names\n",
    "    return {\n",
    "        speakers[0]: s1_filled,\n",
    "        speakers[1]: s2_filled,\n",
    "    }\n",
    "\n",
    "# --- Main nested data structure builder ---\n",
    "all_conversations = {}\n",
    "\n",
    "if DATA_PATH.exists():\n",
    "    for condition_dir in sorted([d for d in DATA_PATH.iterdir() if d.is_dir()]):\n",
    "        condition_name = condition_dir.name\n",
    "        all_conversations[condition_name] = {}\n",
    "\n",
    "        for txt_file in sorted(condition_dir.glob(\"*.txt\")):\n",
    "            text = txt_file.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "            speaker_series = process_conversation(text)\n",
    "            all_conversations[condition_name][txt_file.stem] = speaker_series\n",
    "\n",
    "# --- Example summary output ---\n",
    "for condition, convs in all_conversations.items():\n",
    "    print(f\"\\nCondition: {condition}\")\n",
    "    for conv_name, data in convs.items():\n",
    "        print(f\"  Conversation: {conv_name}\")\n",
    "        for speaker, series in data.items():\n",
    "            print(f\"    {speaker}: {len(series)} words (example: {series[:10]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9db1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def compute_crqa_metrics(s1, s2, min_diag_len=2):\n",
    "    \"\"\"Compute Recurrence Rate and Determinism between two numeric sequences.\"\"\"\n",
    "    s1 = np.asarray(s1)\n",
    "    s2 = np.asarray(s2)\n",
    "\n",
    "    if s1.size == 0 or s2.size == 0:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    R = (s1[:, None] == s2[None, :]).astype(int)\n",
    "    total_points = R.size\n",
    "    recurrent_points = R.sum()\n",
    "    rr = recurrent_points / total_points if total_points > 0 else np.nan\n",
    "\n",
    "    # Determinism: proportion of recurrence points that form diagonals >= min_diag_len\n",
    "    det_points = 0\n",
    "    for k in range(-len(s1) + 1, len(s2)):\n",
    "        diag = np.diagonal(R, offset=k)\n",
    "        count = 0\n",
    "        for v in diag:\n",
    "            if v == 1:\n",
    "                count += 1\n",
    "            else:\n",
    "                if count >= min_diag_len:\n",
    "                    det_points += count\n",
    "                count = 0\n",
    "        if count >= min_diag_len:\n",
    "            det_points += count\n",
    "\n",
    "    det = det_points / recurrent_points if recurrent_points > 0 else np.nan\n",
    "    return float(rr), float(det)\n",
    "\n",
    "\n",
    "# ---- Main CRQA loop ----\n",
    "records = []\n",
    "\n",
    "for condition, convs in all_conversations.items():\n",
    "    for conv_name, data in convs.items():\n",
    "        speakers = list(data.keys())\n",
    "        if len(speakers) != 2:\n",
    "            continue\n",
    "        s1 = np.array(data[speakers[0]])\n",
    "        s2 = np.array(data[speakers[1]])\n",
    "\n",
    "        rr, det = compute_crqa_metrics(s1, s2)\n",
    "        records.append({\n",
    "            \"Condition\": condition,\n",
    "            \"Conversation\": conv_name,\n",
    "            \"Speaker1\": speakers[0],\n",
    "            \"Speaker2\": speakers[1],\n",
    "            \"RR\": rr,\n",
    "            \"DET\": det\n",
    "        })\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd1c515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chatgpt5_chats, 0.00113862462048895, 0.07840616966580977\n",
      "chatgpt5_chats, 0.0012968146371765078, 0.09351145038167939\n",
      "chatgpt5_chats, 0.0013714948616375812, 0.1225296442687747\n",
      "chatgpt5_chats, 0.001549296748320315, 0.14328210213187903\n",
      "chatgpt5_chats, 0.001168065232698824, 0.07410491257285595\n",
      "chatgpt5_chats, 0.0014134137897830702, 0.08840227088402271\n",
      "chatgpt5_chats, 0.001707348952717054, 0.06539074960127592\n",
      "chatgpt5_chats, 0.0012803605093512605, 0.04963235294117647\n",
      "chatgpt5_chats, 0.0012048192771084338, 0.0855421686746988\n",
      "claude_haiku-45_chats, 0.0010421910371570804, 0.0846286701208981\n",
      "claude_haiku-45_chats, 0.001580609095490048, 0.12391033623910336\n",
      "claude_haiku-45_chats, 0.0016128352167313206, 0.07770515613652869\n",
      "claude_haiku-45_chats, 0.0013077498710413321, 0.10617283950617284\n",
      "claude_haiku-45_chats, 0.00168307320957956, 0.12804878048780488\n",
      "claude_haiku-45_chats, 0.0010557726861645969, 0.07830188679245283\n",
      "claude_haiku-45_chats, 0.001499236975518421, 0.11177347242921014\n",
      "claude_haiku-45_chats, 0.001253180839189627, 0.05906735751295337\n",
      "claude_haiku-45_chats, 0.0016683301663439297, 0.10749185667752444\n",
      "human_chats, 0.002554606401384083, 0.19576719576719576\n",
      "human_chats, 0.002836407876867361, 0.07231920199501247\n",
      "human_chats, 0.0021551466098346525, 0.1\n",
      "human_chats, 0.002859967516418332, 0.19135802469135801\n",
      "human_chats, 0.0017794154698916604, 0.13274336283185842\n",
      "human_chats, 0.003544536271808999, 0.14766839378238342\n",
      "human_chats, 0.0026863298703434665, 0.09438775510204081\n",
      "human_chats, 0.0019393889903949208, 0.13157894736842105\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for entry in records:\n",
    "    \n",
    "    print(f\"{entry[\"Condition\"]}, {entry[\"RR\"]}, {entry[\"DET\"]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "393e3690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV file written to: conversation_crqa_data.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "output_file = \"conversation_crqa_data.csv\"\n",
    "\n",
    "# Open file for writing\n",
    "with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # Write header row\n",
    "    writer.writerow([\"Condition\", \"Conversation\", \"Speaker1\", \"Speaker2\", \"RR\", \"DET\"])\n",
    "\n",
    "    # Write each record as a new line\n",
    "    for entry in records:\n",
    "        writer.writerow([\n",
    "            entry[\"Condition\"],\n",
    "            entry[\"Conversation\"],\n",
    "            entry[\"Speaker1\"],\n",
    "            entry[\"Speaker2\"],\n",
    "            entry[\"RR\"],\n",
    "            entry[\"DET\"]\n",
    "        ])\n",
    "\n",
    "print(f\"✅ CSV file written to: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crqa-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
